{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import accumulate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580b195",
   "metadata": {},
   "source": [
    "# Format Data for Neo4j Import\n",
    "\n",
    "In this notebook we format data to load into Neo4j.  Our source will be the training dataset used in the [CIKM Cross-Device Entity Linking Challenge](https://competitions.codalab.org/competitions/11171). The raw dataset can be found [here](https://drive.google.com/file/d/0B7XZSACQf0Kdc3BnZzdEZzR6X0k/view?usp=sharing&resourcekey=0-KcizLV8DrV0jw_VLPNLvxQ).  \n",
    "\n",
    "__To run this notebook please do the following:__\n",
    " - create a subdirectory called `./data`\n",
    " - download the `data-train-dca.zip` file from [here](https://drive.google.com/drive/folders/0B7XZSACQf0KdNXVIUXEyVGlBZnc?resourcekey=0-7ITozWjtDNvBHfTROIfxLg) (~850MB) and unzip into `./data` directory. \n",
    " \n",
    "__Note: Depending on how you set the parameters below, this notebook could take 10 or more minutes to run completely__\n",
    "\n",
    "### Summary \n",
    "This notebook subsamples the raw data based on the `SAMPLE_RATE` defined below and transforms it into five input data sets\n",
    "\n",
    "1. __(Users)__: Nodes Representing Users\n",
    "\n",
    "\n",
    "2. __(Users)-\\[SAME_AS\\]->(Users)__: Relationships representing labeled Entity-Linkage/Alignments between Users.  Will be used for supervised link prediction. \n",
    "\n",
    "\n",
    "3. __(Websites)__: Nodes representing websites. Each URL path from the input dataset will be decomposed into multiple websites to represent the path hierarchy. The depth to consider for the hierarchy is set by the `MAX_URL_SEGMENTS` variable below. Each URL will be split into `MAX_URL_SEGMENTS + 1` website nodes where `MAX_URL_SEGMENTS` website nodes will be used for the hierarchy and the last website node will correspond to the the full url path.  For example, if `MAX_URL_SEGMENTS=3`, then the URL `a/b/c/d/e` will become 4 website nodes identified by the urls `[a, a/b, a/b/c, a/b/c/d/e]` while the website `aa/bb/cc` will become `[aa, aa/bb, aa/bb/cc, aa/bb/cc]`. The website nodes will be deduplicated so each unique url in the hierarchy is only represented by a singe node in the graph. This is done because the full url paths are fairly unique and provide only sparse connectivity between users.  Splitting urls in this way allows us to draw meaningful relationships from which we can engineer features for link prediction. \n",
    "\n",
    "\n",
    "4. __(User)-\\[VISITED\\]->(Website)__: Relationships representing users interacting with websites. The relationships will be weighted by number of visits.\n",
    "\n",
    "\n",
    "5. __(Website)-\\[CHILD_OF\\]->(Website)__: Relationships representing hierarchical structure of websites. i.e. if the URL `a/b/c/d` is split into `[a, a/b, a/b/c, a/b/c/d]` per above rules, then there would be a `CHILD_OF` relationship going from `a/b/c/d -> a/b/c`, `a/b/c -> a/b`, and `a/b -> a`.  This allows for a much more well connected graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b12a39",
   "metadata": {},
   "source": [
    "## Parameters for Sampling and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rate between(0,1) to sample User Ids from facts.json. \n",
    "# Recommend using <=0.1 for quick experimentation\n",
    "SAMPLE_RATE = 0.1\n",
    "\n",
    "# Whether to sample only pairs of nodes that are aligned in the training set. \n",
    "# This will bias the graph to a higher true class ratio if set to True\n",
    "ONLY_SAMPLE_ALIGNED_PAIRS = False\n",
    "\n",
    "# The number of URL segments to consider for the URLs\n",
    "MAX_URL_SEGMENTS = 5\n",
    "\n",
    "# The Random Seed to use throughout\n",
    "RANDOM_SEED = 7474"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fccd0e",
   "metadata": {},
   "source": [
    "## Format Facts (a.k.a Logged Events)\n",
    "\n",
    "We load facts.json to sample from it and shape a unique __userid->eventid__ data frame.\n",
    "The size of the data set may limit the ability to load it all at once depending on your environment so the below example iterates over the json file in chunks.  Feel free to tune the paramters so it is optimized for your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def extract_values_from_facts(row):\n",
    "    return row.facts['fid'], row.facts['ts']\n",
    "\n",
    "def format_facts(raw_df):\n",
    "    df = raw_df.explode(\"facts\")\n",
    "    df[['fid','ts']] = df.apply(lambda row: extract_values_from_facts(row), axis=1, result_type ='expand')\n",
    "    #df.drop(columns = [\"facts\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646315e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled Entity-Linkages/Alignments\n",
    "raw_train_df = pd.read_csv('./data/data-train-dca/train.csv', header=0, names=['uid1', 'uid2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ea6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#iterative sampling of facts.json\n",
    "n = 0\n",
    "facts_dfs = []\n",
    "chunk_size = 5000\n",
    "if ONLY_SAMPLE_ALIGNED_PAIRS:\n",
    "    sample_train_df = raw_train_df.sample(frac=SAMPLE_RATE, random_state = RANDOM_SEED)\n",
    "    unique_aligned_uids = pd.concat([sample_train_df.uid1, sample_train_df.uid2], ignore_index=True) \\\n",
    "    .drop_duplicates().tolist()\n",
    "    with pd.read_json('./data/data-train-dca/facts.json', lines=True, chunksize=chunk_size) as reader:\n",
    "        for chunk_df in reader:\n",
    "            is_aligned_indicator = chunk_df.uid.isin(unique_aligned_uids)\n",
    "            # join on alignments present, sub-sample, and format\n",
    "            facts_dfs.append(format_facts(chunk_df[is_aligned_indicator]))\n",
    "            #print progress\n",
    "            n = n + chunk_size\n",
    "            # print(n)\n",
    "else:\n",
    "    unique_aligned_uids = pd.concat([raw_train_df.uid1, raw_train_df.uid2], ignore_index=True) \\\n",
    "    .drop_duplicates().tolist()\n",
    "    with pd.read_json('./data/data-train-dca/facts.json', lines=True, chunksize=chunk_size) as reader:\n",
    "        for chunk_df in reader:\n",
    "            is_aligned_indicator = chunk_df.uid.isin(unique_aligned_uids)\n",
    "            # join on alignments present, sub-sample, and format\n",
    "            chunk_df_aligned = chunk_df[is_aligned_indicator].sample(frac=SAMPLE_RATE, random_state = RANDOM_SEED)\n",
    "            facts_dfs.append(format_facts(chunk_df_aligned ))\n",
    "            # anti-join on alignments present\n",
    "            chunk_df_not_aligned = chunk_df[~is_aligned_indicator].sample(frac=SAMPLE_RATE, random_state = RANDOM_SEED)\n",
    "            facts_dfs.append(format_facts(chunk_df_not_aligned ))\n",
    "            #print progress\n",
    "            n = n + chunk_size\n",
    "            # print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_df = pd.concat(facts_dfs, ignore_index=True)\n",
    "facts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf587df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_facts_df = facts_df.drop_duplicates(subset = [\"uid\", \"fid\", \"ts\"])\n",
    "clean_facts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef273031",
   "metadata": {},
   "source": [
    "## Subsample Training Alignments based on Facts Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid_df = clean_facts_df.uid.drop_duplicates().to_frame(name=\"uid\")\n",
    "## merge on both alignment columns\n",
    "alignment_df = raw_train_df.merge(unique_uid_df, left_on=\"uid1\", right_on=\"uid\") \\\n",
    ".merge(unique_uid_df, left_on=\"uid2\", right_on=\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed666c",
   "metadata": {},
   "source": [
    "## Join URLS to Capture Site Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd31a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv('./data/data-train-dca/urls.csv', header=0, names=['fid', 'url'])\n",
    "# urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that neither fid nor url have duplicate values\n",
    "print(urls_df.fid.duplicated().sum())\n",
    "print(urls_df.url.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d9c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge facts and url visits on fid.\n",
    "user_web_df = clean_facts_df.merge(urls_df, on=\"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the urls out by path segments so there are seperate columns for the path hierarchy\n",
    "def agg_path(s1, s2):\n",
    "    return s1 + '/' + s2\n",
    "\n",
    "def url_parse(url):\n",
    "    query_pos = url.find('?')\n",
    "    if query_pos != -1:\n",
    "        url = url[0:query_pos]\n",
    "    url_path = url.split('/')\n",
    "    \n",
    "    url_path_length = len(url_path)\n",
    "\n",
    "    if url_path_length>=MAX_URL_SEGMENTS:\n",
    "        url_agg_paths = list(accumulate(url_path[0:MAX_URL_SEGMENTS], agg_path))\n",
    "    else:\n",
    "        url_agg_paths_staging = list(accumulate(url_path[0:url_path_length], agg_path))\n",
    "        url_agg_paths = url_agg_paths_staging + [url_agg_paths_staging[-1]] * (MAX_URL_SEGMENTS - url_path_length)\n",
    "        \n",
    "    return  url_agg_paths + [url_path_length]   \n",
    "\n",
    "url_segment_names = [\"url_segment_\" + str(x) for x in np.arange(0, MAX_URL_SEGMENTS, 1)]\n",
    "url_new_column_names = url_segment_names + [\"url_path_depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba51bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_web_df[url_new_column_names] = \\\n",
    "    user_web_df.apply(lambda row: url_parse(row.url), axis=1, result_type ='expand')\n",
    "#user_web_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad90c92",
   "metadata": {},
   "source": [
    "## Format and Write CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a395c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './data/clean'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "    print(f'Created new directory: {output_path}')\n",
    "else:\n",
    "    print(f'Directory {output_path} already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Users\n",
    "user_web_df['uid'].drop_duplicates().to_csv(f'{output_path}/users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (User)-[SAME_AS]->(User)\n",
    "alignment_df.to_csv(f'{output_path}/user_alignments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Website)-[:CHILD_OF]->(Website)\n",
    "\n",
    "all_url_column_names = url_segment_names + ['url']\n",
    "\n",
    "web_hierarchy_dfs = []\n",
    "\n",
    "for i in range(len(all_url_column_names)-1):\n",
    "    url1 = all_url_column_names[i]\n",
    "    url2 = all_url_column_names[i + 1]\n",
    "\n",
    "    web_hier_segment_df = user_web_df[[url1, url2]]\n",
    "    web_hier_segment_df = web_hier_segment_df.rename(columns={url1:\"url1\", url2:\"url2\"})\n",
    "    web_hier_segment_df = web_hier_segment_df[web_hier_segment_df.url1 != web_hier_segment_df.url2]\n",
    "    web_hierarchy_dfs.append(web_hier_segment_df)\n",
    "\n",
    "web_hierarchy_df = pd.concat(web_hierarchy_dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "web_hierarchy_df.to_csv(f'{output_path}/web_hierarchy.csv', index=False)\n",
    "#web_hierarchy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Websites\n",
    "websites_dfs = []\n",
    "for url in all_url_column_names:  \n",
    "    websites_df_staging = user_web_df[[url, \"url_path_depth\"]]\n",
    "    if url != 'url':\n",
    "        websites_df_staging = websites_df_staging.rename(columns={url:\"url\"})\n",
    "        websites_df_staging = websites_df_staging.drop(columns=\"url_path_depth\")\n",
    "        websites_df_staging = websites_df_staging.assign(url_path_depth=int(url[-1]) + 1)\n",
    "    websites_dfs.append(websites_df_staging)  \n",
    "    \n",
    "websites_df = pd.concat(websites_dfs, ignore_index=True).drop_duplicates(subset=\"url\")\n",
    "websites_df.to_csv(f'{output_path}/websites.csv', index=False)\n",
    "#websites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (User)-[:VISITED]->(Website)\n",
    "def sorted_list(x):\n",
    "    res = x.tolist()\n",
    "    res.sort()\n",
    "    return res\n",
    "\n",
    "users_web_visits_df = user_web_df[['uid','url','fid','ts']]\n",
    "users_web_visits_df = user_web_df[['uid','url','facts','fid','ts']].groupby(['uid','url']) \\\n",
    "      .agg({'facts': lambda x: x.tolist(),'fid': lambda x: set(x.tolist()), 'ts': lambda x: sorted_list(x)}) \\\n",
    "      .reset_index()\n",
    "users_web_visits_df['number_of_visits'] = users_web_visits_df.fid.apply(lambda x: len(x))\n",
    "users_web_visits_df.to_csv(f'{output_path}/user_website_visits.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
